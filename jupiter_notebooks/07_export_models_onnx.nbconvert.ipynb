{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0dafaf0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-17T03:01:11.508970Z",
     "iopub.status.busy": "2025-12-17T03:01:11.508698Z",
     "iopub.status.idle": "2025-12-17T03:01:12.410036Z",
     "shell.execute_reply": "2025-12-17T03:01:12.408844Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX Runtime version: 1.23.2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from skl2onnx import convert_sklearn\n",
    "from skl2onnx.common.data_types import FloatTensorType\n",
    "import onnxruntime as rt\n",
    "\n",
    "try:\n",
    "    from onnxmltools.convert import convert_xgboost, convert_lightgbm\n",
    "    xgb_lgbm_available = True\n",
    "except:\n",
    "    xgb_lgbm_available = False\n",
    "    print(\"onnxmltools nao disponivel, XGBoost e LightGBM nao serao convertidos\")\n",
    "\n",
    "print(f\"ONNX Runtime version: {rt.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b14c7441",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-17T03:01:12.411982Z",
     "iopub.status.busy": "2025-12-17T03:01:12.411729Z",
     "iopub.status.idle": "2025-12-17T03:01:12.416678Z",
     "shell.execute_reply": "2025-12-17T03:01:12.415654Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape X_train: (472, 27)\n",
      "Shape X_test: (119, 27)\n"
     ]
    }
   ],
   "source": [
    "models_dir = '../models/'\n",
    "onnx_dir = '../models/onnx/'\n",
    "os.makedirs(onnx_dir, exist_ok=True)\n",
    "\n",
    "X_train = np.load('../data/processed/X_train.npy')\n",
    "X_test = np.load('../data/processed/X_test.npy')\n",
    "y_test = np.load('../data/processed/y_test.npy')\n",
    "\n",
    "print(f\"Shape X_train: {X_train.shape}\")\n",
    "print(f\"Shape X_test: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8e8265",
   "metadata": {},
   "source": [
    "## Funcao para converter modelo sklearn para ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b71cb511",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-17T03:01:12.418289Z",
     "iopub.status.busy": "2025-12-17T03:01:12.418115Z",
     "iopub.status.idle": "2025-12-17T03:01:12.421986Z",
     "shell.execute_reply": "2025-12-17T03:01:12.421131Z"
    }
   },
   "outputs": [],
   "source": [
    "def convert_model_to_onnx(model, model_name, n_features, output_path):\n",
    "    initial_type = [('float_input', FloatTensorType([None, n_features]))]\n",
    "    \n",
    "    try:\n",
    "        onnx_model = convert_sklearn(model, initial_types=initial_type, target_opset=12)\n",
    "        \n",
    "        with open(output_path, \"wb\") as f:\n",
    "            f.write(onnx_model.SerializeToString())\n",
    "        \n",
    "        print(f\"Modelo {model_name} convertido com sucesso\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao converter {model_name}: {str(e)}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c20792",
   "metadata": {},
   "source": [
    "## Funcao para testar modelo ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a761885",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-17T03:01:12.423625Z",
     "iopub.status.busy": "2025-12-17T03:01:12.423467Z",
     "iopub.status.idle": "2025-12-17T03:01:12.427638Z",
     "shell.execute_reply": "2025-12-17T03:01:12.426820Z"
    }
   },
   "outputs": [],
   "source": [
    "def test_onnx_model(onnx_path, sklearn_model, X_sample):\n",
    "    sess = rt.InferenceSession(onnx_path)\n",
    "    input_name = sess.get_inputs()[0].name\n",
    "    label_name = sess.get_outputs()[0].name\n",
    "    \n",
    "    X_sample_float = X_sample.astype(np.float32)\n",
    "    \n",
    "    pred_onnx = sess.run([label_name], {input_name: X_sample_float})[0]\n",
    "    \n",
    "    pred_sklearn = sklearn_model.predict_proba(X_sample)[:, 1]\n",
    "    \n",
    "    if hasattr(pred_onnx[0], '__len__'):\n",
    "        pred_onnx_proba = pred_onnx[:, 1]\n",
    "    else:\n",
    "        pred_onnx_proba = pred_onnx\n",
    "    \n",
    "    diff = np.abs(pred_sklearn - pred_onnx_proba).mean()\n",
    "    \n",
    "    print(f\"Diferenca media entre predicoes: {diff:.6f}\")\n",
    "    \n",
    "    if diff < 0.001:\n",
    "        print(\"Modelo ONNX validado com sucesso\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"Atencao: diferenca significativa detectada\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f0f601",
   "metadata": {},
   "source": [
    "## Carregar e converter modelos individuais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41085332",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-17T03:01:12.429293Z",
     "iopub.status.busy": "2025-12-17T03:01:12.429132Z",
     "iopub.status.idle": "2025-12-17T03:01:12.432814Z",
     "shell.execute_reply": "2025-12-17T03:01:12.431977Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de modelos encontrados: 36\n",
      "Modelos:\n",
      "  - ensemble_rank_averaging.pkl\n",
      "  - svm_rbf_bayessearch.pkl\n",
      "  - ensemble_voting_optimized.pkl\n",
      "  - advanced_xgboost_top15.pkl\n",
      "  - logistic_regression_optimized.pkl\n",
      "  - logistic_regression_bayessearch.pkl\n",
      "  - svm_rbf_optimized.pkl\n",
      "  - ensemble_weighted_rank.pkl\n",
      "  - advanced_xgboost_all.pkl\n",
      "  - advanced_mlp_all.pkl\n",
      "  - advanced_lightgbm_top15.pkl\n",
      "  - bayesopt_mlp_rfe.pkl\n",
      "  - bayesopt_gradient_boosting_all.pkl\n",
      "  - advanced_mlp_top10.pkl\n",
      "  - svm_linear_baseline.pkl\n",
      "  - bayesopt_mlp_all.pkl\n",
      "  - naive_bayes_baseline.pkl\n",
      "  - ensemble_baseline_advanced.pkl\n",
      "  - logistic_regression_baseline.pkl\n",
      "  - random_forest_bayessearch.pkl\n",
      "  - bayesopt_gradient_boosting_rfe.pkl\n",
      "  - advanced_lightgbm_top10.pkl\n",
      "  - bayesopt_lightgbm_rfe.pkl\n",
      "  - advanced_lightgbm_all.pkl\n",
      "  - bayesopt_elastic_net_all.pkl\n",
      "  - random_forest_baseline.pkl\n",
      "  - random_forest_optimized.pkl\n",
      "  - bayesopt_xgboost_rfe.pkl\n",
      "  - bayesopt_svm_rbf_rfe.pkl\n",
      "  - advanced_mlp_top15.pkl\n",
      "  - bayesopt_xgboost_all.pkl\n",
      "  - bayesopt_elastic_net_rfe.pkl\n",
      "  - advanced_xgboost_top10.pkl\n",
      "  - bayesopt_lightgbm_all.pkl\n",
      "  - naive_bayes_optimized.pkl\n",
      "  - bayesopt_svm_rbf_all.pkl\n"
     ]
    }
   ],
   "source": [
    "model_files = [f for f in os.listdir(models_dir) if f.endswith('.pkl')]\n",
    "\n",
    "print(f\"Total de modelos encontrados: {len(model_files)}\")\n",
    "print(\"Modelos:\")\n",
    "for f in model_files:\n",
    "    print(f\"  - {f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3cd68d7e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-17T03:01:12.434681Z",
     "iopub.status.busy": "2025-12-17T03:01:12.434514Z",
     "iopub.status.idle": "2025-12-17T03:01:12.441424Z",
     "shell.execute_reply": "2025-12-17T03:01:12.440604Z"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "expected 'except' or 'finally' block (3897015551.py, line 58)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 58\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mconversion_results.append({\u001b[39m\n    ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m expected 'except' or 'finally' block\n"
     ]
    }
   ],
   "source": [
    "conversion_results = []\n",
    "\n",
    "for model_file in model_files:\n",
    "    if 'ensemble' in model_file.lower():\n",
    "        print(f\"Pulando {model_file} (ensemble nao suportado)\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\nProcessando: {model_file}\")\n",
    "    \n",
    "    model_path = os.path.join(models_dir, model_file)\n",
    "    \n",
    "    try:\n",
    "        with open(model_path, 'rb') as f:\n",
    "            model_data = pickle.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"  Erro ao carregar: {str(e)[:100]}\")\n",
    "        conversion_results.append({\n",
    "            'model': model_file.replace('.pkl', ''),\n",
    "            'converted': False,\n",
    "            'validated': False,\n",
    "            'path': None\n",
    "        })\n",
    "        continue\n",
    "    \n",
    "    if hasattr(model_data, 'best_estimator_'):\n",
    "        model = model_data.best_estimator_\n",
    "        print(f\"  Extraindo best_estimator_\")\n",
    "    elif hasattr(model_data, 'predict_proba'):\n",
    "        model = model_data\n",
    "    elif isinstance(model_data, dict) and 'model' in model_data:\n",
    "        model = model_data['model']\n",
    "    else:\n",
    "        print(f\"  Formato nao reconhecido, pulando\")\n",
    "        continue\n",
    "    \n",
    "    model_type = type(model).__name__\n",
    "    if 'XGB' in model_type or 'LightGBM' in model_type:\n",
    "        print(f\"  Tipo {model_type} - requer onnxmltools, pulando\")\n",
    "        conversion_results.append({\n",
    "            'model': model_file.replace('.pkl', ''),\n",
    "            'converted': False,\n",
    "            'validated': False,\n",
    "            'path': None\n",
    "        })\n",
    "        continue\n",
    "    \n",
    "    n_features = X_train.shape[1]\n",
    "    model_name = model_file.replace('.pkl', '')\n",
    "    onnx_path = os.path.join(onnx_dir, f\"{model_name}.onnx\")\n",
    "    \n",
    "    success = convert_model_to_onnx(model, model_name, n_features, onnx_path)\n",
    "    \n",
    "    if success:\n",
    "        try:\n",
    "            X_sample = X_test[:10]\n",
    "            validation = test_onnx_model(onnx_path, model, X_sample)\n",
    "        \n",
    "        conversion_results.append({\n",
    "            'model': model_name,\n",
    "            'converted': True,\n",
    "            'validated': validation,\n",
    "            'path': onnx_path\n",
    "        })\n",
    "    else:\n",
    "        conversion_results.append({\n",
    "            'model': model_name,\n",
    "            'converted': False,\n",
    "            'validated': False,\n",
    "            'path': None\n",
    "        })\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Conversao finalizada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0c8476",
   "metadata": {},
   "source": [
    "## Resultados da conversao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5056bf07",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-17T03:01:12.443092Z",
     "iopub.status.busy": "2025-12-17T03:01:12.442916Z",
     "iopub.status.idle": "2025-12-17T03:01:12.576850Z",
     "shell.execute_reply": "2025-12-17T03:01:12.575886Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'conversion_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m df_results = pd.DataFrame(\u001b[43mconversion_results\u001b[49m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mResumo:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTotal de modelos processados: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df_results)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'conversion_results' is not defined"
     ]
    }
   ],
   "source": [
    "df_results = pd.DataFrame(conversion_results)\n",
    "\n",
    "print(\"\\nResumo:\")\n",
    "print(f\"Total de modelos processados: {len(df_results)}\")\n",
    "print(f\"Convertidos com sucesso: {df_results['converted'].sum()}\")\n",
    "print(f\"Validados com sucesso: {df_results['validated'].sum()}\")\n",
    "\n",
    "print(\"\\nDetalhes:\")\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c241c0",
   "metadata": {},
   "source": [
    "## Salvar informacoes dos modelos ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "891f5c2c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-17T03:01:12.579121Z",
     "iopub.status.busy": "2025-12-17T03:01:12.578954Z",
     "iopub.status.idle": "2025-12-17T03:01:12.597887Z",
     "shell.execute_reply": "2025-12-17T03:01:12.596974Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'conversion_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m model_info = {\n\u001b[32m      2\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mn_features\u001b[39m\u001b[33m'\u001b[39m: X_train.shape[\u001b[32m1\u001b[39m],\n\u001b[32m      3\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mfeature_names_path\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33m../data/processed/feature_names.txt\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mmodels\u001b[39m\u001b[33m'\u001b[39m: \u001b[43mconversion_results\u001b[49m,\n\u001b[32m      5\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mpreprocessing\u001b[39m\u001b[33m'\u001b[39m: {\n\u001b[32m      6\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mscaler_path\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33m../data/processed/scaler.pkl\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      7\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mlabel_encoder_path\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33m../data/processed/label_encoder.pkl\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      8\u001b[39m     }\n\u001b[32m      9\u001b[39m }\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os.path.join(onnx_dir, \u001b[33m'\u001b[39m\u001b[33mmodel_info.pkl\u001b[39m\u001b[33m'\u001b[39m), \u001b[33m'\u001b[39m\u001b[33mwb\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     12\u001b[39m     pickle.dump(model_info, f)\n",
      "\u001b[31mNameError\u001b[39m: name 'conversion_results' is not defined"
     ]
    }
   ],
   "source": [
    "model_info = {\n",
    "    'n_features': X_train.shape[1],\n",
    "    'feature_names_path': '../data/processed/feature_names.txt',\n",
    "    'models': conversion_results,\n",
    "    'preprocessing': {\n",
    "        'scaler_path': '../data/processed/scaler.pkl',\n",
    "        'label_encoder_path': '../data/processed/label_encoder.pkl'\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(os.path.join(onnx_dir, 'model_info.pkl'), 'wb') as f:\n",
    "    pickle.dump(model_info, f)\n",
    "\n",
    "print(\"Informacoes dos modelos salvas em model_info.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f53fec",
   "metadata": {},
   "source": [
    "## Exemplo de uso do modelo ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec2a5537",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-17T03:01:12.599785Z",
     "iopub.status.busy": "2025-12-17T03:01:12.599622Z",
     "iopub.status.idle": "2025-12-17T03:01:12.624217Z",
     "shell.execute_reply": "2025-12-17T03:01:12.622155Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m successful_models = \u001b[43mdf_results\u001b[49m[df_results[\u001b[33m'\u001b[39m\u001b[33mvalidated\u001b[39m\u001b[33m'\u001b[39m] == \u001b[38;5;28;01mTrue\u001b[39;00m]\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(successful_models) > \u001b[32m0\u001b[39m:\n\u001b[32m      4\u001b[39m     example_model_path = successful_models.iloc[\u001b[32m0\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mpath\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[31mNameError\u001b[39m: name 'df_results' is not defined"
     ]
    }
   ],
   "source": [
    "successful_models = df_results[df_results['validated'] == True]\n",
    "\n",
    "if len(successful_models) > 0:\n",
    "    example_model_path = successful_models.iloc[0]['path']\n",
    "    print(f\"Testando modelo: {successful_models.iloc[0]['model']}\")\n",
    "    \n",
    "    sess = rt.InferenceSession(example_model_path)\n",
    "    input_name = sess.get_inputs()[0].name\n",
    "    label_name = sess.get_outputs()[0].name\n",
    "    \n",
    "    print(f\"\\nInput name: {input_name}\")\n",
    "    print(f\"Input shape: {sess.get_inputs()[0].shape}\")\n",
    "    print(f\"Output name: {label_name}\")\n",
    "    print(f\"Output shape: {sess.get_outputs()[0].shape}\")\n",
    "    \n",
    "    X_example = X_test[:5].astype(np.float32)\n",
    "    predictions = sess.run([label_name], {input_name: X_example})[0]\n",
    "    \n",
    "    print(f\"\\nPredicoes para 5 exemplos:\")\n",
    "    if hasattr(predictions[0], '__len__') and len(predictions[0]) == 2:\n",
    "        proba_class_1 = predictions[:, 1]\n",
    "        print(f\"Probabilidade de conversao: {proba_class_1}\")\n",
    "    else:\n",
    "        print(f\"Predicoes: {predictions}\")\n",
    "else:\n",
    "    print(\"Nenhum modelo validado disponivel para exemplo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a88878a",
   "metadata": {},
   "source": [
    "## Criar arquivo de metadados para a API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31057d57",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-17T03:01:12.627177Z",
     "iopub.status.busy": "2025-12-17T03:01:12.626864Z",
     "iopub.status.idle": "2025-12-17T03:01:12.655604Z",
     "shell.execute_reply": "2025-12-17T03:01:12.654490Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'conversion_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[33m'\u001b[39m\u001b[33m../data/processed/feature_names.txt\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m      2\u001b[39m     feature_names = [line.strip() \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f.readlines()]\n\u001b[32m      4\u001b[39m api_metadata = {\n\u001b[32m      5\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mtask\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mbinary_classification\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      6\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mproblem\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mPredicao de conversao MCI para Demencia em 3 anos\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      7\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mtarget_classes\u001b[39m\u001b[33m'\u001b[39m: [\u001b[33m'\u001b[39m\u001b[33mNao-Conversor\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mConversor\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m      8\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mn_features\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mlen\u001b[39m(feature_names),\n\u001b[32m      9\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mfeature_names\u001b[39m\u001b[33m'\u001b[39m: feature_names,\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mmodels_available\u001b[39m\u001b[33m'\u001b[39m: [m[\u001b[33m'\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[43mconversion_results\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m m[\u001b[33m'\u001b[39m\u001b[33mvalidated\u001b[39m\u001b[33m'\u001b[39m]],\n\u001b[32m     11\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mbest_model\u001b[39m\u001b[33m'\u001b[39m: successful_models.iloc[\u001b[32m0\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(successful_models) > \u001b[32m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     12\u001b[39m     \u001b[33m'\u001b[39m\u001b[33monnx_directory\u001b[39m\u001b[33m'\u001b[39m: onnx_dir\n\u001b[32m     13\u001b[39m }\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjson\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os.path.join(onnx_dir, \u001b[33m'\u001b[39m\u001b[33mapi_metadata.json\u001b[39m\u001b[33m'\u001b[39m), \u001b[33m'\u001b[39m\u001b[33mw\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[31mNameError\u001b[39m: name 'conversion_results' is not defined"
     ]
    }
   ],
   "source": [
    "with open('../data/processed/feature_names.txt', 'r') as f:\n",
    "    feature_names = [line.strip() for line in f.readlines()]\n",
    "\n",
    "api_metadata = {\n",
    "    'task': 'binary_classification',\n",
    "    'problem': 'Predicao de conversao MCI para Demencia em 3 anos',\n",
    "    'target_classes': ['Nao-Conversor', 'Conversor'],\n",
    "    'n_features': len(feature_names),\n",
    "    'feature_names': feature_names,\n",
    "    'models_available': [m['model'] for m in conversion_results if m['validated']],\n",
    "    'best_model': successful_models.iloc[0]['model'] if len(successful_models) > 0 else None,\n",
    "    'onnx_directory': onnx_dir\n",
    "}\n",
    "\n",
    "import json\n",
    "with open(os.path.join(onnx_dir, 'api_metadata.json'), 'w') as f:\n",
    "    json.dump(api_metadata, f, indent=2)\n",
    "\n",
    "print(\"Metadados para API salvos em api_metadata.json\")\n",
    "print(f\"\\nModelos disponiveis para a API: {len(api_metadata['models_available'])}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
